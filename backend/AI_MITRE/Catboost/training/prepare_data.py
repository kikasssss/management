#ph√¢n lo·∫°i d·ªØ li·ªáu t·ª´ data ƒë·ªÉ train
import os
import pandas as pd
import numpy as np

# ========= 1) C·∫•u h√¨nh =========
DATA_DIR = "data"                             # th∆∞ m·ª•c ch·ª©a c√°c file csv ngu·ªìn
OUTPUT_FILE = "combined_logs_minimal.csv"     # file g·ªôp ƒë·∫ßu ra

# C√°c c·ªôt G·ªêC c·∫ßn l·∫•y t·ª´ d·ªØ li·ªáu
SOURCE_COLS = [
    "conn_state", "duration", "history",
    "src_port_zeek", "dest_port_zeek",
    "orig_bytes", "resp_bytes",
    "orig_pkts", "resp_pkts",
    "proto", "service",
    "ts",
    "label_tactic", "label_technique"
]

# Mapping sang schema TRAIN (ECS-like)
RENAME_DICT = {
    "conn_state": "network.state",
    "duration": "event.duration",
    "history": "network.history",
    "src_port_zeek": "source.port",
    "dest_port_zeek": "destination.port",
    "orig_bytes": "source.bytes",
    "resp_bytes": "destination.bytes",
    "orig_pkts": "source.packets",
    "resp_pkts": "destination.packets",
    "proto": "network.transport",
    "service": "network.service",
    "ts": "@timestamp",
    "label_tactic": "threat.tactic.name",
    "label_technique": "threat.technique.name"
}

# Th·ª© t·ª± c·ªôt ƒë√≠ch t·ªëi gi·∫£n ƒë·ªÉ train
DEST_COL_ORDER = [
    "network.state", "network.history",
    "network.transport", "network.service",
    "source.port", "destination.port",
    "event.duration",
    "source.bytes", "destination.bytes",
    "source.packets", "destination.packets",
    "@timestamp",
    "threat.tactic.name", "threat.technique.name"
]

# ========= 2) ƒê·ªçc & g·ªôp =========
csv_files = [os.path.join(DATA_DIR, f) for f in os.listdir(DATA_DIR) if f.endswith(".csv")]

dfs = []
for path in csv_files:
    try:
        df = pd.read_csv(path, low_memory=False)

        # Th√™m c·ªôt thi·∫øu ƒë·ªÉ ƒë·ªìng b·ªô schema
        for col in SOURCE_COLS:
            if col not in df.columns:
                df[col] = np.nan

        # Gi·ªØ ƒë√∫ng th·ª© t·ª± c·ªôt g·ªëc
        df = df[SOURCE_COLS]
        dfs.append(df)
        print(f"‚úÖ Loaded: {path} ({df.shape[0]} rows)")
    except Exception as e:
        print(f"‚ö†Ô∏è Error reading {path}: {e}")

if not dfs:
    raise SystemExit("‚ùå Kh√¥ng t√¨m th·∫•y CSV h·ª£p l·ªá trong th∆∞ m·ª•c 'data'.")

df_all = pd.concat(dfs, ignore_index=True)
print(f"\nüìä T·ªïng c·ªông {df_all.shape[0]} d√≤ng, {df_all.shape[1]} c·ªôt (tr∆∞·ªõc chu·∫©n h√≥a)")

# ========= 3) √âp ki·ªÉu nh·∫π ƒë·ªÉ nh·∫•t qu√°n =========
# C·ªïng, bytes, packets, duration ‚Üí numeric
num_cols = ["duration", "src_port_zeek", "dest_port_zeek",
            "orig_bytes", "resp_bytes", "orig_pkts", "resp_pkts"]
for c in num_cols:
    if c in df_all.columns:
        df_all[c] = pd.to_numeric(df_all[c], errors="coerce")

# ========= 4) ƒê·ªïi t√™n c·ªôt sang schema TRAIN =========
df_all.rename(columns=RENAME_DICT, inplace=True)

# ========= 5) X·ª≠ l√Ω tr√πng c·ªôt / r·ªóng =========
df_all = df_all.loc[:, ~df_all.columns.duplicated()]
df_all.dropna(how="all", inplace=True)

# ========= 6) Th√™m c·ªôt thi·∫øu (n·∫øu c·∫ßn) v√† s·∫Øp x·∫øp l·∫°i th·ª© t·ª± =========
for col in DEST_COL_ORDER:
    if col not in df_all.columns:
        df_all[col] = np.nan
df_all = df_all[DEST_COL_ORDER]

# ========= 7) (Tu·ª≥ ch·ªçn) th√™m ƒë·∫∑c tr∆∞ng th·ªùi gian =========
# N·∫øu timestamp l√† ISO nh∆∞ 2024-11-05T10:00:00.646Z, c√≥ th·ªÉ t·∫°o gi·ªù/ng√†y
USE_TIME_FEATURES = False  # ƒë·∫∑t True n·∫øu mu·ªën th√™m 'hour', 'weekday', 'is_weekend'

if USE_TIME_FEATURES and "@timestamp" in df_all.columns:
    ts = pd.to_datetime(df_all["@timestamp"], errors="coerce", utc=True)
    df_all["hour"] = ts.dt.hour
    df_all["weekday"] = ts.dt.weekday
    df_all["is_weekend"] = df_all["weekday"].isin([5, 6]).astype(int)
    print("üïí ƒê√£ th√™m c√°c ƒë·∫∑c tr∆∞ng th·ªùi gian: hour, weekday, is_weekend")

# ========= 8) Ghi file =========
df_all.to_csv(OUTPUT_FILE, index=False)
print(f"\n‚úÖ ƒê√£ l∆∞u file t·ªëi gi·∫£n cho train AI MITRE: {OUTPUT_FILE}")
print("üìä Shape:", df_all.shape)
